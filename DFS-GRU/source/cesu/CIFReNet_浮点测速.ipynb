{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import operator\n",
    "import torch.nn as nn\n",
    "from functools import reduce\n",
    "from torch.autograd import Variable\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from IPython.display import Image\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "from torch.autograd import Variable\n",
    "from collections import OrderedDict\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os\n",
    "# import cv2\n",
    "import six\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "import numbers\n",
    "import warnings\n",
    "import collections\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "# from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from PIL import ImageOps\n",
    "import scipy.misc as misc\n",
    "from torch.utils import data\n",
    "from datetime import datetime\n",
    "import torch.nn.functional as F\n",
    "from __future__ import division\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import lr_scheduler\n",
    "from collections import OrderedDict\n",
    "import torchvision.transforms as tfs\n",
    "# from tensorboardX import SummaryWriter\n",
    "from torchvision import datasets, models\n",
    "import torchvision.transforms.functional as tf\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from torchvision.transforms import ToTensor, ToPILImage\n",
    "from torchvision.transforms import Compose, CenterCrop, Normalize, Resize, Pad\n",
    "\n",
    "# from functools import partial\n",
    "# try:\n",
    "#     import pydensecrf.densecrf as dcrf\n",
    "# except:\n",
    "#     print(\"Failed to import pydensecrf, CRF post-processing will not work\")\n",
    "\n",
    "count_ops = 0\n",
    "count_params = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.2.1+a4fc05a'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_num_gen(gen):\n",
    "    return sum(1 for x in gen)\n",
    "\n",
    "\n",
    "def is_pruned(layer):\n",
    "    try:\n",
    "        layer.mask\n",
    "        return True\n",
    "    except AttributeError:\n",
    "        return False\n",
    "\n",
    "\n",
    "def is_leaf(model):\n",
    "    return get_num_gen(model.children()) == 0\n",
    "\n",
    "\n",
    "def get_layer_info(layer):\n",
    "    layer_str = str(layer)\n",
    "    type_name = layer_str[:layer_str.find('(')].strip()\n",
    "    return type_name\n",
    "\n",
    "\n",
    "def get_layer_param(model):\n",
    "    return sum([reduce(operator.mul, i.size(), 1) for i in model.parameters()])\n",
    "\n",
    "\n",
    "### The input batch size should be 1 to call this function\n",
    "def measure_layer(layer, x):\n",
    "    global count_ops, count_params\n",
    "    delta_ops = 0\n",
    "    delta_params = 0\n",
    "    multi_add = 1\n",
    "    type_name = get_layer_info(layer)\n",
    "\n",
    "    ### ops_conv\n",
    "    if type_name in ['Conv2d','ConvTranspose2d']:\n",
    "        out_h = int((x.size()[2] + 2 * layer.padding[0] - layer.kernel_size[0]) /\n",
    "                    layer.stride[0] + 1)\n",
    "        out_w = int((x.size()[3] + 2 * layer.padding[1] - layer.kernel_size[1]) /\n",
    "                    layer.stride[1] + 1)\n",
    "        delta_ops = layer.in_channels * layer.out_channels * layer.kernel_size[0] *  \\\n",
    "                layer.kernel_size[1] * out_h * out_w / layer.groups * multi_add\n",
    "        delta_params = get_layer_param(layer)\n",
    "\n",
    "    ### ops_nonlinearity\n",
    "    elif type_name in ['ReLU', 'PReLU','ReLU6', 'LeakyReLU', 'Sigmoid']:\n",
    "        delta_ops = x.numel()\n",
    "        delta_params = get_layer_param(layer)\n",
    "\n",
    "    ### ops_pooling\n",
    "    elif type_name in ['AvgPool2d']:\n",
    "        in_w = x.size()[2]\n",
    "        kernel_ops = layer.kernel_size * layer.kernel_size\n",
    "        out_w = int((in_w + 2 * layer.padding - layer.kernel_size) / layer.stride + 1)\n",
    "        out_h = int((in_w + 2 * layer.padding - layer.kernel_size) / layer.stride + 1)\n",
    "        delta_ops = x.size()[0] * x.size()[1] * out_w * out_h * kernel_ops\n",
    "        delta_params = get_layer_param(layer)\n",
    "\n",
    "    elif type_name in ['AdaptiveAvgPool2d']:\n",
    "        delta_ops = x.size()[0] * x.size()[1] * x.size()[2] * x.size()[3]\n",
    "        delta_params = get_layer_param(layer)\n",
    "\n",
    "    ### ops_linear\n",
    "    elif type_name in ['Linear']:\n",
    "        weight_ops = layer.weight.numel() * multi_add\n",
    "        bias_ops = layer.bias.numel()\n",
    "        delta_ops = x.size()[0] * (weight_ops + bias_ops)\n",
    "        delta_params = get_layer_param(layer)\n",
    "\n",
    "    ### ops_nothing\n",
    "    elif type_name in ['BatchNorm2d', 'Dropout2d', 'DropChannel',\"ShuffleBlock\",\n",
    "                       'Dropout', 'InPlaceABN', 'InPlaceABNSync', 'Upsample', 'MaxPool2d']:\n",
    "        delta_params = get_layer_param(layer)\n",
    "\n",
    "    ### unknown layer type\n",
    "    else:\n",
    "        raise TypeError('unknown layer type: %s' % type_name)\n",
    "\n",
    "    count_ops += delta_ops\n",
    "    count_params += delta_params\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def measure_model(model, H, W):\n",
    "    global count_ops, count_params\n",
    "    count_ops = 0\n",
    "    count_params = 0\n",
    "    data = Variable(torch.zeros(1, 3, H, W).cuda())\n",
    "\n",
    "    def should_measure(x):\n",
    "        return is_leaf(x) or is_pruned(x)\n",
    "\n",
    "    def modify_forward(model):\n",
    "        for child in model.children():\n",
    "            if should_measure(child):\n",
    "                def new_forward(m):\n",
    "                    def lambda_forward(x):\n",
    "                        measure_layer(m, x)\n",
    "                        return m.old_forward(x)\n",
    "                    return lambda_forward\n",
    "                child.old_forward = child.forward\n",
    "                child.forward = new_forward(child)\n",
    "            else:\n",
    "                modify_forward(child)\n",
    "\n",
    "    def restore_forward(model):\n",
    "        for child in model.children():\n",
    "            # leaf node\n",
    "            if is_leaf(child) and hasattr(child, 'old_forward'):\n",
    "                child.forward = child.old_forward\n",
    "                child.old_forward = None\n",
    "            else:\n",
    "                restore_forward(child)\n",
    "\n",
    "    modify_forward(model)\n",
    "    model.forward(data)\n",
    "    restore_forward(model)\n",
    "\n",
    "    return count_ops, count_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#加载预训练的mobilenetv2\n",
    "def conv_bn(inp, oup, stride):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU6(inplace=True)\n",
    "    )\n",
    "\n",
    "\n",
    "def conv_1x1_bn(inp, oup):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.ReLU6(inplace=True)\n",
    "    )\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(self, inp, oup, stride, expand_ratio, dalited):\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.stride = stride\n",
    "        self.d = dalited\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        self.use_res_connect = self.stride == 1 and inp == oup\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            # pw\n",
    "            nn.Conv2d(inp, inp * expand_ratio, 1, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(inp * expand_ratio),\n",
    "            nn.ReLU6(inplace=True),\n",
    "            # dw\n",
    "            nn.Conv2d(inp * expand_ratio, inp * expand_ratio, 3, stride, padding=self.d, dilation=self.d, groups=inp * expand_ratio, bias=False),\n",
    "            nn.BatchNorm2d(inp * expand_ratio),\n",
    "            nn.ReLU6(inplace=True),\n",
    "            # pw-linear\n",
    "            nn.Conv2d(inp * expand_ratio, oup, 1, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(oup),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_res_connect:\n",
    "            return x + self.conv(x)\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(self, n_class=1000, input_size=224, width_mult=1.):\n",
    "        super(MobileNetV2, self).__init__()\n",
    "        # setting of inverted residual blocks\n",
    "        self.interverted_residual_setting = [\n",
    "            # t, c, n, s, d\n",
    "            [1, 16, 1, 1, 1],    # 1/2\n",
    "            [6, 24, 2, 2, 1],    # 1/4\n",
    "            [6, 32, 3, 2, 1],    # 1/8\n",
    "            [6, 64, 4, 1, 2],    # 1/8\n",
    "            [6, 96, 3, 1, 3],    # 1/8\n",
    "            [6, 160, 3, 1, 5],   # 1/8\n",
    "            [6, 320, 1, 1, 7],  # 1/8\n",
    "        ]\n",
    "\n",
    "        # building first layer\n",
    "        assert input_size % 32 == 0\n",
    "        input_channel = int(32 * width_mult)\n",
    "        self.last_channel = int(1280 * width_mult) if width_mult > 1.0 else 1280\n",
    "        self.features = [conv_bn(3, input_channel, 2)]\n",
    "        # building inverted residual blocks\n",
    "        for t, c, n, s ,d in self.interverted_residual_setting:\n",
    "            output_channel = int(c * width_mult)\n",
    "            for i in range(n):\n",
    "                if i == 0:\n",
    "                    self.features.append(InvertedResidual(input_channel, output_channel, s, t,d))\n",
    "                else:\n",
    "                    self.features.append(InvertedResidual(input_channel, output_channel, 1, t,d))\n",
    "                input_channel = output_channel\n",
    "        # building last several layers\n",
    "        self.features.append(conv_1x1_bn(input_channel, self.last_channel))\n",
    "        self.features.append(nn.AvgPool2d(input_size/32))\n",
    "        # make it nn.Sequential\n",
    "        self.features = nn.Sequential(*self.features)\n",
    "\n",
    "        # building classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(self.last_channel, n_class),\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(-1, self.last_channel)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                n = m.weight.size(1)\n",
    "                m.weight.data.normal_(0, 0.01)\n",
    "                m.bias.data.zero_()\n",
    "                \n",
    "                \n",
    "net = MobileNetV2(n_class=1000)\n",
    "mobilenet_v2 = list(net.features.children())\n",
    "len(mobilenet_v2)\n",
    "\n",
    "def channel_shuffle(x, groups):\n",
    "    batchsize, num_channels, height, width = x.data.size()\n",
    "    assert (num_channels % groups == 0)\n",
    "    channels_per_group = num_channels // groups\n",
    "    # reshape\n",
    "    x = x.view(batchsize, groups, channels_per_group, height, width)\n",
    "    # transpose\n",
    "    x = torch.transpose(x, 1, 2).contiguous()\n",
    "    # flatten\n",
    "    x = x.view(batchsize, -1, height, width)\n",
    "    return x\n",
    "\n",
    "class ShuffleBlock(nn.Module):\n",
    "    def __init__(self, groups):\n",
    "        super(ShuffleBlock, self).__init__()\n",
    "        self.groups = groups\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''Channel shuffle: [N,C,H,W] -> [N,g,C/g,H,W] -> [N,C/g,g,H,w] -> [N,C,H,W]'''\n",
    "        N,C,H,W = x.size()\n",
    "        g = self.groups\n",
    "        return x.view(N,g,int(C/g),H,W).permute(0,2,1,3,4).contiguous().view(N,C,H,W)\n",
    "\n",
    "\n",
    "class ABN(nn.Sequential):\n",
    "    def __init__(self, num_features):\n",
    "        super(ABN, self).__init__(OrderedDict([\n",
    "            (\"bn\",  nn.BatchNorm2d(num_features,eps=1e-05, momentum=0.1, affine=True)),\n",
    "            (\"act\", nn.PReLU(num_features))\n",
    "        ]))\n",
    "\n",
    "class DSP(nn.Module):\n",
    "    def __init__(self, inplanes, outplanes, c_tag=0.2, groups=4, dilation=(1,2,3,4)):\n",
    "        super(DSP, self).__init__()\n",
    "        \n",
    "        self.out_c = round(c_tag * outplanes)\n",
    "        \n",
    "        self.down = nn.Sequential(\n",
    "                nn.Conv2d(inplanes, self.out_c, 1, stride=1, groups=groups, bias=False),\n",
    "                ABN(self.out_c)\n",
    "          )\n",
    "        \n",
    "        self.pool =  nn.Sequential(\n",
    "                                nn.AdaptiveAvgPool2d((1, 1)),\n",
    "                                nn.Conv2d(self.out_c, self.out_c, 1, stride=1),\n",
    "                                nn.BatchNorm2d(self.out_c,eps=1e-05, momentum=0.1, affine=True)\n",
    "            )\n",
    "        \n",
    "        self.branch_1 = nn.Sequential(\n",
    "                nn.Conv2d(self.out_c, self.out_c, kernel_size=3, padding=dilation[0], dilation = dilation[0],groups=self.out_c, bias=False),\n",
    "                ABN(self.out_c),\n",
    "                nn.Conv2d(self.out_c, self.out_c, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(self.out_c,eps=1e-05, momentum=0.1, affine=True)\n",
    "        )\n",
    "        self.branch_2 = nn.Sequential(\n",
    "                nn.Conv2d(self.out_c, self.out_c, kernel_size=3, padding=dilation[1],dilation = dilation[1],groups=self.out_c, bias=False),\n",
    "                ABN(self.out_c),\n",
    "                nn.Conv2d(self.out_c, self.out_c, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(self.out_c,eps=1e-05, momentum=0.1, affine=True)\n",
    "            )\n",
    "        self.branch_3 = nn.Sequential(\n",
    "                nn.Conv2d(self.out_c, self.out_c, kernel_size=3, padding=dilation[2],dilation = dilation[2],groups=self.out_c, bias=False),\n",
    "                ABN(self.out_c),\n",
    "                nn.Conv2d(self.out_c, self.out_c, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(self.out_c,eps=1e-05, momentum=0.1, affine=True)\n",
    "        )\n",
    "        \n",
    "        self.branch_4 = nn.Sequential(\n",
    "                nn.Conv2d(self.out_c, self.out_c, kernel_size=3, padding=dilation[3],dilation = dilation[3],groups=self.out_c, bias=False),\n",
    "                ABN(self.out_c),\n",
    "                nn.Conv2d(self.out_c, self.out_c, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(self.out_c,eps=1e-05, momentum=0.1, affine=True)\n",
    "        )\n",
    "       \n",
    "        self.groups = groups\n",
    "        \n",
    "        self.module_act = nn.PReLU(outplanes)\n",
    "      \n",
    "    def forward(self, x):\n",
    "        input_x = x\n",
    "        x_size = x.size()\n",
    "        x= self.down(x)\n",
    "        branch_1 = self.branch_1(x)\n",
    "        branch_2 = self.branch_2(x)\n",
    "        branch_3 = self.branch_3(x)\n",
    "        branch_4 = self.branch_4(x)\n",
    "        pool = F.upsample(self.pool(x),x_size[2:], mode= \"bilinear\")\n",
    "        out = channel_shuffle(torch.cat((branch_1,branch_2,branch_3,branch_4,pool), 1), self.groups)\n",
    "        \n",
    "        if out.size() == input_x.size():\n",
    "            out = out + input_x\n",
    "        return self.module_act(out)\n",
    "    \n",
    "class MCIM(nn.Module):\n",
    "    def __init__(self, in_chs = 320, out_chs = 320):\n",
    "        super(MCIM, self).__init__()\n",
    "        \n",
    "        self.pool =  nn.Sequential(\n",
    "                                nn.AdaptiveAvgPool2d((1, 1)),\n",
    "                                nn.Conv2d(in_chs, 80, 1, stride=1),\n",
    "                                nn.BatchNorm2d(80),\n",
    "                                nn.LeakyReLU(0.1)\n",
    "            )\n",
    "        \n",
    "        self.conv_small = DSP(in_chs, out_chs, dilation = (1,2,3,5))\n",
    "        \n",
    "        self.conv_middle = DSP(out_chs, out_chs, dilation = (7,9,11,13))\n",
    "\n",
    "        self.conv_larger = DSP(out_chs, out_chs, dilation = (17,19,21,23))\n",
    "   \n",
    "    def forward(self, x):\n",
    "        x_size = x.size()\n",
    "        small = self.conv_small(x)\n",
    "        middle = self.conv_middle(small)\n",
    "        larger = self.conv_larger(middle)\n",
    "        pool = F.upsample(self.pool(x),x_size[2:], mode= \"bilinear\")\n",
    "        output = small+middle+larger\n",
    "        output = torch.cat([output,pool],1)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "#语义分割模型 mobilenet_v2\n",
    "def initialize_weights(*models):\n",
    "    for model in models:\n",
    "        for module in model.modules():\n",
    "            if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
    "                nn.init.kaiming_normal(module.weight)\n",
    "                if module.bias is not None:\n",
    "                    module.bias.data.zero_()\n",
    "            elif isinstance(module, nn.BatchNorm2d):\n",
    "                module.weight.data.fill_(1)\n",
    "                module.bias.data.zero_()\n",
    "\n",
    "class Attention_Block(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(Attention_Block, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        self.channel_excitation = nn.Sequential(nn.Linear(channel, int(channel//reduction)),\n",
    "                                                nn.ReLU(inplace=True),                                             \n",
    "                                                nn.Linear(int(channel//reduction), channel),\n",
    "                                                nn.Sigmoid())\n",
    "    def forward(self, x):\n",
    "        bahs, chs, _, _ = x.size()\n",
    "        chn_se = self.avg_pool(x).view(bahs, chs)        \n",
    "        return self.channel_excitation(chn_se).view(bahs, chs, 1, 1)\n",
    "\n",
    "                \n",
    "class M2_semantic(nn.Module):\n",
    "    def __init__(self, num_classes=19):\n",
    "        super(M2_semantic, self).__init__()\n",
    "        \n",
    "        # building inverted residual blocks\n",
    "        self.mod1 = mobilenet_v2[0]\n",
    "        self.mod2 = mobilenet_v2[1]\n",
    "        self.mod3 = nn.Sequential(mobilenet_v2[2],mobilenet_v2[3])\n",
    "        self.mod4 = nn.Sequential(mobilenet_v2[4],mobilenet_v2[5],mobilenet_v2[6])\n",
    "        self.mod5 = nn.Sequential(mobilenet_v2[7],mobilenet_v2[8],mobilenet_v2[9],mobilenet_v2[10])\n",
    "        self.mod6 = nn.Sequential(mobilenet_v2[11],mobilenet_v2[12],mobilenet_v2[13])\n",
    "        self.mod7 = nn.Sequential(mobilenet_v2[14],mobilenet_v2[15],mobilenet_v2[16])\n",
    "        self.mod8 = mobilenet_v2[17]\n",
    "        \n",
    "        self.LRM =  nn.Sequential(\n",
    "                        nn.Conv2d(32, 32, kernel_size=3, padding = 1, groups = 32), \n",
    "                        ABN(32),\n",
    "                        nn.Conv2d(32, 160, kernel_size=1),    \n",
    "                        ABN(160)\n",
    "                            )\n",
    "        \n",
    "        self.multi_scale = MCIM(320, 320)\n",
    "        \n",
    "        self.attention = Attention_Block(160)\n",
    "        \n",
    "        self.final = nn.Sequential(\n",
    "            nn.Conv2d(400, 256, kernel_size = 1, stride = 1,groups =16),\n",
    "            ShuffleBlock(16),\n",
    "            nn.BatchNorm2d(256,eps=1e-05, momentum=0.1, affine=True),\n",
    "            nn.LeakyReLU(0.1),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Conv2d(256, num_classes, kernel_size=1)\n",
    "        )\n",
    "        initialize_weights(self.LRM, self.attention, self.multi_scale, self.final)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_size = x.size()  \n",
    "        stg0 = self.mod1(x)         # torch.Size([1, 32, 112, 224])\n",
    "        \n",
    "        stg1 = self.mod2(stg0)      # torch.Size([1, 16, 112, 224])\n",
    "        stg2 = self.mod3(stg1)      # torch.Size([1, 24, 56, 112])     \n",
    "        stg3 = self.mod4(stg2)  # torch.Size([1, 32, 28, 56]) \n",
    "        \n",
    "        LRM = self.LRM(stg3)  # torch.Size([1, 32, 28, 56])\n",
    "        \n",
    "        stg4 = self.mod5(stg3)  # torch.Size([1, 64, 28, 56])\n",
    "        stg5 = self.mod6(stg4) # torch.Size([1, 96, 28, 56])   \n",
    "        stg6 = self.mod7(stg5)  # torch.Size([1, 160, 28, 56])\n",
    "        \n",
    "        attention = self.attention(stg6)\n",
    "        \n",
    "        stg6 = torch.mul(attention,LRM) + stg6\n",
    "        \n",
    "        stg7 = self.mod8(stg6)  # torch.Size([1, 320, 28, 56])\n",
    "\n",
    "        multi_scale = self.multi_scale(stg7)\n",
    "        \n",
    "#         print(modified_aspp.size())\n",
    "        out = self.final(multi_scale)\n",
    "        \n",
    "        return F.upsample(out, x_size[2:], mode='bilinear')\n",
    "    \n",
    "model = M2_semantic(num_classes=21).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLOPs: 7267744548.0, Params: 1950415\n"
     ]
    }
   ],
   "source": [
    "net_h, net_w = 360,640\n",
    "count_ops, count_params = measure_model(model, net_h, net_w)\n",
    "print('FLOPs: {}, Params: {}'.format(count_ops, count_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLOPs: 8260974628.0, Params: 1950415\n"
     ]
    }
   ],
   "source": [
    "net_h, net_w = 512,512\n",
    "count_ops, count_params = measure_model(model, net_h, net_w)\n",
    "print('FLOPs: {}, Params: {}'.format(count_ops, count_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLOPs: 16277781732.0, Params: 1950415\n"
     ]
    }
   ],
   "source": [
    "net_h, net_w = 713,713\n",
    "count_ops, count_params = measure_model(model, net_h, net_w)\n",
    "print('FLOPs: {}, Params: {}'.format(count_ops, count_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLOPs: 14431301668.0, Params: 1950415\n"
     ]
    }
   ],
   "source": [
    "net_h, net_w = 1024,448\n",
    "count_ops, count_params = measure_model(model, net_h, net_w)\n",
    "print('FLOPs: {}, Params: {}'.format(count_ops, count_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FLOPs: 16482744356.0, Params: 1950415\n"
     ]
    }
   ],
   "source": [
    "net_h, net_w = 1024,512\n",
    "count_ops, count_params = measure_model(model, net_h, net_w)\n",
    "print('FLOPs: {}, Params: {}'.format(count_ops, count_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
